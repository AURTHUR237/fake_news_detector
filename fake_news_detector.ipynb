{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "fake news detector"
      ],
      "metadata": {
        "id": "j1LKF8I41ygQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are using A Million News Headlines dataset to implement a fake news headlines detection model. Where the A Million News Headlines dataset will be labeled as real news headlines. we will also use two fake news headline datasets on Kaggle from Fake and real news and Getting Real about Fake News ."
      ],
      "metadata": {
        "id": "I5A4VOKV_J7I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# imports for reading our dataset\n",
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "hxsMJLMu9oZP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load the dataset"
      ],
      "metadata": {
        "id": "XXa1HC-T_lqB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Reading the three Dataset\n",
        "Headlines = pd.read_csv('/content/drive/MyDrive/fakenewsdata/abcnews-date-text.csv', usecols =[\"headline_text\"]).dropna()\n",
        "Headlines1 = pd.read_csv('/content/drive/MyDrive/fakenewsdata/Fake.csv', usecols =[\"title\"]).dropna()\n",
        "Headlines2 = pd.read_csv('/content/drive/MyDrive/fakenewsdata/True.csv', usecols =[\"title\"]).dropna()"
      ],
      "metadata": {
        "id": "xq5qVDrm_hSa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing duplicated headlines from the data\n",
        "Headlines = Headlines.drop_duplicates('headline_text')\n",
        "Headlines1 = Headlines1.drop_duplicates('title')\n",
        "Headlines2 = Headlines2.drop_duplicates('title')"
      ],
      "metadata": {
        "id": "WU2dsBQzDW7f"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Renaming the dataframe columns and Combining all datasets\n",
        "Headlines1 = Headlines1.rename(columns={'title': 'headline_text'})\n",
        "Headlines2 = Headlines2.rename(columns={'title': 'headline_text'})"
      ],
      "metadata": {
        "id": "aXfqIJ02DwPZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "data labelling"
      ],
      "metadata": {
        "id": "GFYACzm6EAAh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Creating lable for datasets\n",
        "#million-headlines dataset will be used as real headlines\n",
        "#fake-and-real-news-dataset & fake-news dataset will be used as fake headlines\n",
        "Headlines['fake'] = 0\n",
        "Headlines1['fake'] = 1\n",
        "Headlines2['fake'] = 1"
      ],
      "metadata": {
        "id": "1tXolT28D5Oe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combining dataset\n",
        "#Downsize million-headlines dataset to first 50K rows\n",
        "data = pd.concat([Headlines[:50000],Headlines1,Headlines2])\n",
        "print('Training dataset contains: {} Real headlines and {} Fake headlines.'.format(50000,len(Headlines1)+len(Headlines2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DUqU0i2EGAI",
        "outputId": "e4b6b30e-4a25-4806-9b60-f078d67564cb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training dataset contains: 50000 Real headlines and 38729 Fake headlines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "so the dataset contains 50000 real headlines and 38729 fake headlines"
      ],
      "metadata": {
        "id": "dXqNPeIjEUVr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "data preprocessing"
      ],
      "metadata": {
        "id": "zIb28apUEiLw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgzacBfpE4cy",
        "outputId": "9a6a9308-8fc4-4e59-de72-41617dc85aa1"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing important liabraries\n",
        "import gensim\n",
        "import nltk as nl\n",
        "from sklearn.feature_extraction import _stop_words\n",
        "\n",
        "\n",
        "nltk_stopwords = nl.corpus.stopwords.words('english')\n",
        "gensim_stopwords = gensim.parsing.preprocessing.STOPWORDS\n",
        "sklearn_stopwords = _stop_words.ENGLISH_STOP_WORDS\n",
        "combined_stopwords = sklearn_stopwords.union(nltk_stopwords,gensim_stopwords)"
      ],
      "metadata": {
        "id": "LyrSO7dVEh1B"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('NLTK has {} stop words'.format(len(nltk_stopwords)))\n",
        "print('Gensim has {} stop words'.format(len(gensim_stopwords)))\n",
        "print('Sklearn has {} stop words'.format(len(sklearn_stopwords)))\n",
        "print('Combined stopwords list has {} stop words'.format(len(combined_stopwords)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5J5gDAaQEOsF",
        "outputId": "d28e97ec-a9b9-4606-81e2-17bbd12cb3c7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK has 179 stop words\n",
            "Gensim has 337 stop words\n",
            "Sklearn has 318 stop words\n",
            "Combined stopwords list has 390 stop words\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter_stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "HQ-2avg7Fi1V"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['headline_text'] = data['headline_text'].apply(lambda x: x.lower())\n",
        "data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word.isalpha()]))\n",
        "data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([porter_stemmer.stem(word) for word in x.split()]))\n",
        "data['headline_text'] = data['headline_text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (combined_stopwords)]))"
      ],
      "metadata": {
        "id": "Jl0ROWOaFobl"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "splitting the data\n",
        "training data is 80 percent\n",
        "testing data is 20 percent"
      ],
      "metadata": {
        "id": "HmukNxMCGRne"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test=train_test_split(data['headline_text'], data['fake'], test_size=0.2, random_state=7)"
      ],
      "metadata": {
        "id": "XHVzKkG_Fs1j"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Construct models with TF-IDF"
      ],
      "metadata": {
        "id": "cVFrDLQ2G3M_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "we are going to use the following models and test their accuracy\n",
        "\n",
        "\n",
        "DecisionTreeClassifier,\n",
        "LogisticRegression,\n",
        "SVC,\n",
        "KNeighborsClassifier and Naive_Bayes"
      ],
      "metadata": {
        "id": "XU4ZP669JtLu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Dropout, Conv1D, MaxPooling1D, Flatten, Embedding, GlobalMaxPooling1D\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "HDmYtq39Gvoq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(tokenizer = word_tokenize, max_features = 300)\n",
        "tfidf_train = tfidf_vectorizer.fit_transform(x_train)\n",
        "tfidf_test = tfidf_vectorizer.transform(x_test)\n",
        "tfidf_features = tfidf_vectorizer.get_feature_names_out()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0tofLuIInBm",
        "outputId": "a355a214-fc4d-45e9-bb5a-3849223dcc59"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/feature_extraction/text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dt = DecisionTreeClassifier()\n",
        "rf = RandomForestClassifier()\n",
        "svc = SVC(kernel='linear')\n",
        "knn = KNeighborsClassifier()\n",
        "nb = MultinomialNB()\n",
        "lg=  LogisticRegression()\n",
        "\n",
        "dt.fit(tfidf_train, y_train)\n",
        "rf.fit(tfidf_train, y_train)\n",
        "svc.fit(tfidf_train, y_train)\n",
        "knn.fit(tfidf_train, y_train)\n",
        "nb.fit(tfidf_train, y_train)\n",
        "lg.fit(tfidf_train, y_train)"
      ],
      "metadata": {
        "id": "TimizMleIoBD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Testing Acc. of Decision Tree: {} %\".format(round(dt.score(tfidf_test, y_test) * 100, 2)))\n",
        "print (\"Testing Acc. of Random Forest: {} %\".format(round(rf.score(tfidf_test, y_test) * 100, 2)))\n",
        "print (\"Testing Acc. of SVC: {} %\".format(round(svc.score(tfidf_test, y_test) * 100, 2)))\n",
        "print (\"Testing Acc. of K-NN: {} %\".format(round(knn.score(tfidf_test, y_test) * 100, 2)))\n",
        "print (\"Testing Acc. of Naive Bayesian: {} %\".format(round(nb.score(tfidf_test, y_test) * 100, 2)))\n",
        "print (\"Testing Acc. of Logistic Regression: {} %\".format(round(lg.score(tfidf_test, y_test) * 100, 2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7AmKfw1I4el",
        "outputId": "daa71dc1-4ad7-487d-b9e6-5d96cc8a349e"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing Acc. of Decision Tree: 83.75 %\n",
            "Testing Acc. of Random Forest: 85.33 %\n",
            "Testing Acc. of SVC: 84.55 %\n",
            "Testing Acc. of K-NN: 82.25 %\n",
            "Testing Acc. of Naive Bayesian: 83.79 %\n",
            "Testing Acc. of Logistic Regression: 85.11 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kwA9fippN4mx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}